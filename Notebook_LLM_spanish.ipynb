{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQg5PxhPSh13"
      },
      "source": [
        "<div >\n",
        "<img src = \"../banner.jpg\" />\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2evzlHn-Sh15"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github//ignaciomsarmiento/BDML_202402/blob/main/Lecture15/Notebook_LLM.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnQ1GgeuSh16"
      },
      "source": [
        "# Construyamos un GPT desde cero"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYaHPz2MSh18"
      },
      "source": [
        "## Chat GPT: Entendiendo los Modelos de Lenguaje y su Funcionamiento Interno\n",
        "\n",
        "Chat GPT es lo que llamamos un **modelo de lenguaje**, ya que se encarga de modelar secuencias de palabras, caracteres o, de forma más general, **tokens**. Su principal habilidad radica en comprender cómo las palabras se relacionan y siguen unas a otras en un idioma, en este caso, el inglés.\n",
        "\n",
        "Desde su perspectiva, lo que realmente hace es **completar secuencias**: tú le proporcionas el inicio de una secuencia y el modelo genera el resto basándose en patrones que ha aprendido durante su entrenamiento. Por esta razón, se clasifica como un modelo de lenguaje.\n",
        "\n",
        "Sin embargo, más allá de esta funcionalidad básica, es importante explorar cómo funciona \"bajo el capó\" Chat GPT, es decir, entender los componentes internos que hacen posible su desempeño. La base de este modelo es una red neuronal que modela la secuencia de palabras, y su funcionamiento se deriva de un artículo científico clave titulado **\"Attention is All You Need\"** publicado en 2017. Este trabajo marcó un antes y un después en la inteligencia artificial al introducir la **arquitectura Transformer**, la piedra angular de sistemas como Chat GPT.\n",
        "\n",
        "<div >\n",
        "<img src = \"figs/transformer.png\" />\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1kU2pSVSh19"
      },
      "source": [
        "El término GPT significa **\"Generative Pre-trained Transformer\"** (Transformer Generativo Preentrenado), destacando los tres elementos clave que lo definen:\n",
        "\n",
        "1. **Generative (Generativo):** Es capaz de generar texto de forma autónoma y coherente.\n",
        "2. **Pre-trained (Preentrenado):** Ha sido entrenado previamente con grandes cantidades de datos para entender patrones lingüísticos.\n",
        "3. **Transformer:** Utiliza la arquitectura Transformer, que basa su eficacia en un mecanismo conocido como **atención** para procesar y modelar secuencias de texto con gran precisión.\n",
        "\n",
        "Este tutorial se centrará en desglosar cómo la arquitectura Transformer hace posible que Chat GPT modele las secuencias de lenguaje y produzca respuestas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Bx8J5HhE2QC9"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "\n",
        "# read it in to inspect it\n",
        "url = 'https://gist.githubusercontent.com/jsdario/6d6c69398cb0c73111e49f1218960f79/raw/8d4fc4548d437e2a7203a5aeeace5477f598827d/el_quijote.txt'\n",
        "with urllib.request.urlopen(url) as f:\n",
        "    text = f.read().decode('utf-8')  # Decode the bytes to string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DXwAy8J0863x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbf16b86-a683-492c-af11-1b3e9b2c54f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1038397\n"
          ]
        }
      ],
      "source": [
        "# print el numero de caracters unicos en text\n",
        "print(len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwfSa3L3Sh2A",
        "outputId": "da09b201-f34d-419f-c04e-d2eac14a8d41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DON QUIJOTE DE LA MANCHA\n",
            "Miguel de Cervantes Saavedra\n",
            "\n",
            "PRIMERA PARTE\n",
            "CAPÍTULO 1: Que trata de la condición y ejercicio del famoso hidalgo D. Quijote de la Mancha\n",
            "En un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho tiempo que vivía un hidalgo de los de lanza en astillero, adarga antigua, rocín flaco y galgo corredor. Una olla de algo más vaca que carnero, salpicón las más noches, duelos y quebrantos los sábados, lentejas los viernes, algún palomino de añadidura los domingos, consumían las tres partes de su hacienda. El resto della concluían sayo de velarte, calzas de velludo para las fiestas con sus pantuflos de lo mismo, los días de entre semana se honraba con su vellori de lo más fino. Tenía en su casa una ama que pasaba de los cuarenta, y una sobrina que no llegaba a los veinte, y un mozo de campo y plaza, que así ensillaba el rocín como tomaba la podadera. Frisaba la edad de nuestro hidalgo con los cincuenta años, era de complexión recia, seco de carnes, enjuto de rostro; gran madrugador y amigo de la caza. Quieren decir que tenía el sobrenombre de Quijada o Quesada (que en esto hay alguna diferencia en los autores que deste caso escriben), aunque por conjeturas verosímiles se deja entender que se llama Quijana; pero esto importa poco a nuestro cuento; basta que en la narración dél no se salga un punto de la verdad. Es, pues, de saber, que este sobredicho hidalgo, los ratos que estaba ocioso (que eran los más del año) se daba a leer libros de caballerías con tanta afición y gusto, que olvidó casi de todo punto el ejercicio de la caza, y aun la administración de su hacienda; y llegó a tanto su curiosidad y desatino en esto, que vendió muchas hanegas de tierra de sembradura, para comprar libros de caballerías en que leer; y así llevó a su casa todos cuantos pudo haber dellos; y de todos ningunos le parecían tan bien como los que compuso el famoso Feliciano de Silva: porque la claridad de su prosa, y aquellas intrincadas razones suyas, le parecían de perlas; y más cuando llegaba a leer aquellos requiebros y cartas de desafío, donde en muchas partes hallaba escrito: la razón de la sinrazón que a mi razón se hace, de tal manera mi razón enflaquece, que con razón me quejo de la vuestra fermosura, y también cuando leía: los altos cielos que de vuestra divinidad divinamente con las estrellas se fortifican, y os hacen merecedora del merecimiento que merece la vuestra grandeza. Con estas y semejantes razones perdía el pobre caballero el juicio, y desvelábase por entenderlas, y desentrañarles el sentido, que no se lo sacara, ni las entendiera el mismo Aristóteles, si resucitara para sólo ello. No estaba muy bien con las heridas que don Belianis daba y recibía, porque se imaginaba que por grandes maestros que le hubiesen curado, no dejaría de tener el rostro y todo el cuerpo lleno de cicatrices y señales; pero con todo alababa en su autor aquel acabar su libro con la promesa de aqu\n"
          ]
        }
      ],
      "source": [
        "print(text[:3000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gFOcXQ_m83hQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a13b84ae-cd22-42a7-806a-017916aff4eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !\"'(),-.0123456789:;<?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]abcdefghijlmnopqrstuvxyz¡«»¿̀́̃̈–‘’“”\n"
          ]
        }
      ],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ep7KcaZzaM2",
        "outputId": "67dc671d-1924-41e2-c157-be1ab7deba48"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]"
      ],
      "metadata": {
        "id": "7GAITvvGzucc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encode(\"hii there\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xpivI8iz8Zc",
        "outputId": "7ddfd294-dcaa-4925-d91d-6a1de44b04fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "decode = lambda l: ''.join([itos[i] for i in l])"
      ],
      "metadata": {
        "id": "0ULI3STT0Td3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(encode(\"hola\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmnShMX80cCd",
        "outputId": "f4f709ad-8ce6-482a-c8bb-20b220ed8170"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hola\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "uuUnG1bt1XF-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83j8mJYU1lWX",
        "outputId": "a7afc16a-a344-4c0d-9147-c58feee07bf1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1038397]) torch.int64\n",
            "tensor([27, 38, 37,  1, 40, 44, 32, 33, 38, 43, 28,  1, 27, 28,  1, 35, 24,  1,\n",
            "        36, 24, 37, 26, 31, 24,  0, 36, 60, 58, 71, 56, 62,  1, 55, 56,  1, 26,\n",
            "        56, 68, 72, 52, 64, 70, 56, 69,  1, 42, 52, 52, 72, 56, 55, 68, 52,  0,\n",
            "         0, 39, 41, 32, 36, 28, 41, 24,  1, 39, 24, 41, 43, 28,  0, 26, 24, 39,\n",
            "        32, 81, 43, 44, 35, 38,  1, 11, 20,  1, 40, 71, 56,  1, 70, 68, 52, 70,\n",
            "        52,  1, 55, 56,  1, 62, 52,  1, 54, 65, 64, 55, 60, 54, 60, 65, 81, 64,\n",
            "         1, 74,  1, 56, 61, 56, 68, 54, 60, 54, 60, 65,  1, 55, 56, 62,  1, 57,\n",
            "        52, 63, 65, 69, 65,  1, 59, 60, 55, 52, 62, 58, 65,  1, 27,  9,  1, 40,\n",
            "        71, 60, 61, 65, 70, 56,  1, 55, 56,  1, 62, 52,  1, 36, 52, 64, 54, 59,\n",
            "        52,  0, 28, 64,  1, 71, 64,  1, 62, 71, 58, 52, 68,  1, 55, 56,  1, 62,\n",
            "        52,  1, 36, 52, 64, 54, 59, 52,  7,  1, 55, 56,  1, 54, 71, 74, 65,  1,\n",
            "        64, 65, 63, 53, 68, 56,  1, 64, 65,  1, 67, 71, 60, 56, 68, 65,  1, 52,\n",
            "        54, 65, 68, 55, 52, 68, 63, 56,  7,  1, 64, 65,  1, 59, 52,  1, 63, 71,\n",
            "        54, 59, 65,  1, 70, 60, 56, 63, 66, 65,  1, 67, 71, 56,  1, 72, 60, 72,\n",
            "        60, 81, 52,  1, 71, 64,  1, 59, 60, 55, 52, 62, 58, 65,  1, 55, 56,  1,\n",
            "        62, 65, 69,  1, 55, 56,  1, 62, 52, 64, 75, 52,  1, 56, 64,  1, 52, 69,\n",
            "        70, 60, 62, 62, 56, 68, 65,  7,  1, 52, 55, 52, 68, 58, 52,  1, 52, 64,\n",
            "        70, 60, 58, 71, 52,  7,  1, 68, 65, 54, 60, 81, 64,  1, 57, 62, 52, 54,\n",
            "        65,  1, 74,  1, 58, 52, 62, 58, 65,  1, 54, 65, 68, 68, 56, 55, 65, 68,\n",
            "         9,  1, 44, 64, 52,  1, 65, 62, 62, 52,  1, 55, 56,  1, 52, 62, 58, 65,\n",
            "         1, 63, 52, 81, 69,  1, 72, 52, 54, 52,  1, 67, 71, 56,  1, 54, 52, 68,\n",
            "        64, 56, 68, 65,  7,  1, 69, 52, 62, 66, 60, 54, 65, 81, 64,  1, 62, 52,\n",
            "        69,  1, 63, 52, 81, 69,  1, 64, 65, 54, 59, 56, 69,  7,  1, 55, 71, 56,\n",
            "        62, 65, 69,  1, 74,  1, 67, 71, 56, 53, 68, 52, 64, 70, 65, 69,  1, 62,\n",
            "        65, 69,  1, 69, 52, 81, 53, 52, 55, 65, 69,  7,  1, 62, 56, 64, 70, 56,\n",
            "        61, 52, 69,  1, 62, 65, 69,  1, 72, 60, 56, 68, 64, 56, 69,  7,  1, 52,\n",
            "        62, 58, 71, 81, 64,  1, 66, 52, 62, 65, 63, 60, 64, 65,  1, 55, 56,  1,\n",
            "        52, 64, 82, 52, 55, 60, 55, 71, 68, 52,  1, 62, 65, 69,  1, 55, 65, 63,\n",
            "        60, 64, 58, 65, 69,  7,  1, 54, 65, 64, 69, 71, 63, 60, 81, 52, 64,  1,\n",
            "        62, 52, 69,  1, 70, 68, 56, 69,  1, 66, 52, 68, 70, 56, 69,  1, 55, 56,\n",
            "         1, 69, 71,  1, 59, 52, 54, 60, 56, 64, 55, 52,  9,  1, 28, 62,  1, 68,\n",
            "        56, 69, 70, 65,  1, 55, 56, 62, 62, 52,  1, 54, 65, 64, 54, 62, 71, 60,\n",
            "        81, 52, 64,  1, 69, 52, 74, 65,  1, 55, 56,  1, 72, 56, 62, 52, 68, 70,\n",
            "        56,  7,  1, 54, 52, 62, 75, 52, 69,  1, 55, 56,  1, 72, 56, 62, 62, 71,\n",
            "        55, 65,  1, 66, 52, 68, 52,  1, 62, 52, 69,  1, 57, 60, 56, 69, 70, 52,\n",
            "        69,  1, 54, 65, 64,  1, 69, 71, 69,  1, 66, 52, 64, 70, 71, 57, 62, 65,\n",
            "        69,  1, 55, 56,  1, 62, 65,  1, 63, 60, 69, 63, 65,  7,  1, 62, 65, 69,\n",
            "         1, 55, 60, 81, 52, 69,  1, 55, 56,  1, 56, 64, 70, 68, 56,  1, 69, 56,\n",
            "        63, 52, 64, 52,  1, 69, 56,  1, 59, 65, 64, 68, 52, 53, 52,  1, 54, 65,\n",
            "        64,  1, 69, 71,  1, 72, 56, 62, 62, 65, 68, 60,  1, 55, 56,  1, 62, 65,\n",
            "         1, 63, 52, 81, 69,  1, 57, 60, 64, 65,  9,  1, 43, 56, 64, 60, 81, 52,\n",
            "         1, 56, 64,  1, 69, 71,  1, 54, 52, 69, 52,  1, 71, 64, 52,  1, 52, 63,\n",
            "        52,  1, 67, 71, 56,  1, 66, 52, 69, 52, 53, 52,  1, 55, 56,  1, 62, 65,\n",
            "        69,  1, 54, 71, 52, 68, 56, 64, 70, 52,  7,  1, 74,  1, 71, 64, 52,  1,\n",
            "        69, 65, 53, 68, 60, 64, 52,  1, 67, 71, 56,  1, 64, 65,  1, 62, 62, 56,\n",
            "        58, 52, 53, 52,  1, 52,  1, 62, 65, 69,  1, 72, 56, 60, 64, 70, 56,  7,\n",
            "         1, 74,  1, 71, 64,  1, 63, 65, 75, 65,  1, 55, 56,  1, 54, 52, 63, 66,\n",
            "        65,  1, 74,  1, 66, 62, 52, 75, 52,  7,  1, 67, 71, 56,  1, 52, 69, 60,\n",
            "        81,  1, 56, 64, 69, 60, 62, 62, 52, 53, 52,  1, 56, 62,  1, 68, 65, 54,\n",
            "        60, 81, 64,  1, 54, 65, 63, 65,  1, 70, 65, 63, 52, 53, 52,  1, 62, 52,\n",
            "         1, 66, 65, 55, 52, 55, 56, 68, 52,  9,  1, 29, 68, 60, 69, 52, 53, 52,\n",
            "         1, 62, 52,  1, 56, 55, 52, 55,  1, 55, 56,  1, 64, 71, 56, 69, 70, 68,\n",
            "        65,  1, 59, 60, 55, 52, 62, 58, 65,  1, 54, 65, 64,  1, 62, 65, 69,  1,\n",
            "        54, 60, 64, 54, 71, 56, 64, 70, 52,  1, 52, 64, 82, 65, 69,  7,  1, 56,\n",
            "        68, 52,  1, 55, 56,  1, 54, 65, 63, 66, 62, 56, 73, 60, 65, 81, 64,  1,\n",
            "        68, 56, 54, 60, 52,  7,  1, 69, 56, 54])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir en entrenamiento y validation set\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "val_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdxrxZea2Wud",
        "outputId": "70447711-0988-4b34-b194-1f3ac87eb2c2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([62, 60, 53,  ..., 32, 42,  0])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCKtoAdb2kVd",
        "outputId": "a6acc82d-c134-4aef-beb3-62c5ca31d72e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([27, 38, 37,  1, 40, 44, 32, 33, 38])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"cuando el insumo es (x) {context} el objetivo (y): {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StTRGDLH3mX8",
        "outputId": "88776554-827d-4fb0-b7d1-764da45110d1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuando el insumo es (x) tensor([27]) el objetivo (y): 38\n",
            "cuando el insumo es (x) tensor([27, 38]) el objetivo (y): 37\n",
            "cuando el insumo es (x) tensor([27, 38, 37]) el objetivo (y): 1\n",
            "cuando el insumo es (x) tensor([27, 38, 37,  1]) el objetivo (y): 40\n",
            "cuando el insumo es (x) tensor([27, 38, 37,  1, 40]) el objetivo (y): 44\n",
            "cuando el insumo es (x) tensor([27, 38, 37,  1, 40, 44]) el objetivo (y): 32\n",
            "cuando el insumo es (x) tensor([27, 38, 37,  1, 40, 44, 32]) el objetivo (y): 33\n",
            "cuando el insumo es (x) tensor([27, 38, 37,  1, 40, 44, 32, 33]) el objetivo (y): 38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "block_size = 8 # tamaño de contexto\n",
        "batch_size = 4 # numero de secuencias en paralelo"
      ],
      "metadata": {
        "id": "FCAVWIhx4MGB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "9zvZ6C1d4gY5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFDmIybG4iBy",
        "outputId": "5a5b438e-7ef5-415b-dee8-ab8fa4e5a877"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[70, 52, 55,  1, 55, 56,  1, 66],\n",
            "        [ 7,  1, 54, 65, 64,  1, 56, 62],\n",
            "        [60,  1, 52,  1, 63, 60, 81,  1],\n",
            "        [54, 52, 68, 52,  7,  1, 64, 60]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[52, 55,  1, 55, 56,  1, 66, 52],\n",
            "        [ 1, 54, 65, 64,  1, 56, 62,  1],\n",
            "        [ 1, 52,  1, 63, 60, 81,  1, 63],\n",
            "        [52, 68, 52,  7,  1, 64, 60,  1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkiy257w7LIr",
        "outputId": "9715a52d-38b8-42c2-a85a-1a6ac5f44e3b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7bd81e5cad70>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "Tjs5huYl5xiY"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = BigramLanguageModel(vocab_size)\n",
        "logits = m(xb, yb)"
      ],
      "metadata": {
        "id": "kzd1qzTy7Dg4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_EAgaVm94vL",
        "outputId": "2de1938c-a5f0-4d75-c860-6d394ed587c9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "8sLv<.OTM»̃\"DK;C??c\"TŃ6)-HpLL);GhLRN!̀’IJ1PS«L¡X9”:l\";;]:5Vnd»PjYV“Y«\"ITaQ2’̀rE?x”b‘5”c¿o'H7L[cB3;Q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "DvbUU4lz_0uJ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(10000): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHMY3liO_3vI",
        "outputId": "7463b543-bd10-414d-dc3f-2ab6c115b4e0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.233574628829956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4_RGLD1ARjQ",
        "outputId": "d8af93e1-93da-4073-9a61-5f381f32c40a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "N: caro, SAle dono Sa el ]Qunusa es sanzcompomi nte dida la yasueriela W¡cha tre eIAmo vondiece os do cos as horos, da hacise Q ja yen pe. anteta ta, s, pas lomes cárá micora; yobiesañodabrel, pé, ve sís al dí tenójo acovidrlllo e leno vira se sosan queguerigo, de ce –X dejon cor sna, ba durídenta l o; nsOre s scicotea labla dílo y EVX(Anazoronte pesalo, Dosabra desin te bíncú tadabustométadera cegurtená la po a biase deblane y, cahe, litapamprcimide brama e canena ale s si¿mon lo, \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT"
      ],
      "metadata": {
        "id": "ZG34vSojDG2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "import urllib.request\n",
        "\n",
        "# read it in to inspect it\n",
        "url = 'https://gist.githubusercontent.com/jsdario/6d6c69398cb0c73111e49f1218960f79/raw/8d4fc4548d437e2a7203a5aeeace5477f598827d/el_quijote.txt'\n",
        "with urllib.request.urlopen(url) as f:\n",
        "    text = f.read().decode('utf-8')  # Decode the bytes to string\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "id": "lYdr2COLDDj6",
        "outputId": "0d3e37d4-35bc-4025-fe7a-621095fbde30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.807385 M parameters\n",
            "step 0: train loss 4.6409, val loss 4.6399\n",
            "step 500: train loss 1.7349, val loss 1.8019\n",
            "step 1000: train loss 1.2936, val loss 1.4177\n",
            "step 1500: train loss 1.1502, val loss 1.3057\n",
            "step 2000: train loss 1.0702, val loss 1.2670\n",
            "step 2500: train loss 1.0014, val loss 1.2429\n",
            "step 3000: train loss 0.9486, val loss 1.2409\n",
            "step 3500: train loss 0.8958, val loss 1.2557\n",
            "step 4000: train loss 0.8384, val loss 1.2595\n",
            "step 4500: train loss 0.7862, val loss 1.2875\n",
            "step 4999: train loss 0.7351, val loss 1.3091\n",
            "\n",
            "¿Cas lé conciencia de su famosa tiembre Zoraida, no es posible perdido el deseo de saber de firmar y jamás de vuestra presteza. ¿Es verdad sabe, qué parte peligros luema? Cuanto mas vuestra merced decir era los valerosos a felicios? A fe, respondió Don Quijote.\n",
            "-No respondí más -respondió Sancho-: mas en el mundo, cuanto más huele a tomar en el mal aposento, por su parecer el decoro que tiene de hacer vengarnos a saberlo, hasta que, como ¿os hay mi madre dijiste, ¿ño Andarésele que hab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=300)[0].tolist()))"
      ],
      "metadata": {
        "id": "q-tlOFQ3P6GZ",
        "outputId": "5f6a05b8-84cd-44c2-b90e-8fa088e299d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "en efeto, mi testimosa escritura de mi presencia! ¡Mojas, mozas a mi padre. venturas, hablé tan ruindo como a muchos seves días, que no parece si está en las palabras y descubiertos los discursos, hallándonos ejércitos con ellos, si es ser su esposo; porque todos los míos no sere hacer, que de\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=5000)[0].tolist()))"
      ],
      "metadata": {
        "id": "airB-7KfDDxi",
        "outputId": "65277eab-b426-44b8-da3a-12977554e187",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "parándola tiempo gusto, le dijo:\n",
            "–Pues ¿no es persuadizas, y, si este mi amo, señor oidor, que queréis ser afgicar todo tan dedo esta industriba camila; que así volvió Don Quijote y el cantarillo le tomó, aunque eran de los pesqueños altos guardase; y lo pudo hartar en el difunto dineros y mala voz hijo que la hereja hizo, espantada, en los diablos estados, que andaban divididosen con mucha diligencia puso por fuerza. En efecto, la judita, se fue a la porque certaba era su idea Mambrina; y, sin que, a brodas o Anselmo le respondió a Rodrigo por palaba, que si la dio al voces, ya que prestos no fueran; pero si es disculpas de encajo cosa y digno, de quien se haya doblariendo mi vsallor y haciéndolo a su hijo, de dosde lón dellos una avénís, tres disponeían los dos en alto, y crey de las cosas que le entró su parecer, y que no confundía otra vez que oía aber a la nueva, donde lloraba, ni era otro ni mucho tiempo medísimos de palosías, sule y lo cual le vistó sus armas la majada que huía.\n",
            "Con este gigante, que estas y dos hermanos se veían añadidurados, como don Luis le fidaron la causa de mis una de los mejores hicieros; y acudiendo énto, y así, amigo como he dicho, hoy sobe una mujer y cédala, la moza sobra en la misma, y la perdida lo como supla, que no soy el ni opinación consigo mal deseo que reposadamos el Padre, que bien la Túlaga, Anselmo puede hallé adespués mi escudero Sancho. Pues hay que han muertos en mi mudo las faltas y las órdientes que me va un cuita y mita; y el más en aquella noche se hace aloja tan repentino claramente; pero ya tardaré por venganza de alegra contra secreto buscando las demás los palos aventureros de los señores Castilla; la cual estaba hilando para mayores disculparasen don Quijote de la Mancha, porque era niña hasta que hosalían al castillo como una hora tardada. Fosera preguntar puesto un difuz en la más alta fuerza que distiendo: est́ se esparcán que este es bachiller de grande y el de leer, porque en la mulanda en la fuerza de favore, y avíne un necio de pedillo y de las de un cotoroso en la manta, que aún lo supiera por don Quijote desplardo; y de muy buen hombre esté a los buenos ojornos, ha cantado con el de espíro a la señora Dulcinea\n",
            "del Toboso, y décil agravio y de el deseo que hemos visitarme en todos los días. Y ellos, quedaron por coida y entretenerle en la que tenían ver un casallo! Así que le respondió que ha de aguardarle le hallaron preguntas y al nudo se ajenó la gente, y vínosele al caballero humeriero hacia luz de la mano, se le tornó a rey el suelo; mas no podimos llegar a persuadir a su gusto, rogándole con mucho al cabestro le dijo: Por camarillo, con el agustoro de sus parecer (que me llegue le tiene a la mula que imposible puede tenerme a ser el que en sus esposos jamás solozos; y, o, y de decir que quere que algunos hombres libres estaba la juzgar de algunos piensa, no dijo mucho vizcaínano y su compañero, y le hallé cocido y su casa, el cual no halló en si: ¿Por qué tienes, más hermosa señor, dijo Don Quijote. Y hasta más cofar con ahínco, ¡4hora deposital de mí!\n",
            "Antequella nevillos\", quizá mi marido. Y quizá con más versos: díjome Sancho Panza, que por acudir a ver si era por mí estimarle.\n",
            "Pero poco a que palmó don Juan Pandafilando hase este lugar el Bonsado, que a su no eran hurtos, dificultad mejor dinero así sería inveniente historia, y así, cuando llegaron a ellos no fuesen el polvo de sus amigos y los demás caminos, un a sí quisiera ser en qué!\n",
            "-¿Quién estuviera yo un propósito Lotario,\n",
            "y asazado en hora del bálsamo era du negro de Córdoba Redpís, si adonde él no procurábamos, ser eso negro de todos.\n",
            "A lo que se me olvida se entonce se iba Diole mi camarada humbre, por inclinaje y rica, loto\n",
            "Cuerdo el que Tin había de parecer en ello que tenía dormía. Lotario supieron qué templesaron que hasta enecender su a los suyos.\n",
            "Capítulo 16: Desespermos úl rayo para raso, que quizá primero salirn la fe horca, escudo de algún yelmo algunos magamosos, y requiera haberle oído ver si lo fuera doncella, y aun una de rigor os de necerla. Dos con esa arrogando, Sancho Dios, fomenzo, y sin aguardarme, no que la he deseo de tomar a Dios, que por los mozos golpes a estos señores. Y si no fuese pena porque fara el gusto pensar siempre; que por un sabio que jamás ya disco pudie, que ya había volvitado el mejor que tenía de habello. A lo cual respondió: Pero ésta buena fermosa, dijo Andrés. Es, Pas mí sé que de debe ser de vencedor: mirad si eran las horas de aquel mismo que a mi madre después con mi hija hijo; si no acaso se debía y de hallar un a oro: mira en que otra me falta le parece y bien volvó las cartas que soce él le lleva y le agradaban, y que faltóme solicitas a pocos mil segundos de hir los vientes, hacia un redens y unos de los fuentes del brachón, o deleitas y compa al pie de la que llevaba ulamenta; mas la cual se quedó dormido, se supo con m\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}